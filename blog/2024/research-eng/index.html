<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  ACSL  Autonomous Computing Systems Lab


  | Research Statement

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />
-->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⭐</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2024/research-eng/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JG6HYJP07P"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-JG6HYJP07P');
</script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>
    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
        <span class="font-weight-bold" id="main_name">ACSL</span><span id="head_name"></span>
      </a>
      
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          <!-- Blog -->
          
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- -->
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <!-- <li class="nav-item "> -->
          <li class="nav-item ">
              <a class="nav-link" href="/people/">
                People
                
              </a>
          </li>
          
          
          
          <!-- <li class="nav-item "> -->
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          <!-- <li class="nav-item "> -->
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>

<script>
  if ((window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth) > 600) {
    document.getElementById("head_name").innerText = document.getElementById('head_name').innerHTML + ' ' + '' + 'Autonomous Computing Systems Lab';
  }
</script>

    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Research Statement</h1>
    <p class="post-meta"></p>
  </header>

  <article class="post-content">
    <p>We design and build autonomous robot systems that process various types of sensor data <strong>to realize meaningful services for humans</strong>. With an unprecedented scale of available data and computational resources, robotics systems could better understand the surrounding environments and improve user experience in human-robot interaction (HRI) scenarios. In various domains, our research aims to design robotics systems that 1) extract both semantic and physical information from sensor data for understanding the surrounding environments (perception), 2) safely reach the goal point in unstructured environments (navigation) and 3) elicit user behavior, intention or message from sensor data for promoting natural HRI experiences. Our research attempts to take a step toward the long-dreamed goal of replacing repetitive and dangerous tasks with robotics systems and natural interaction between human and robotics systems.</p>

<p>Specifically, our research has focused on making robotics systems for three crucial components of <strong>robot-in-the-loop research scenarios</strong>:</p>

<ul>
  <li>multi-modal perception and navigation</li>
  <li>service robots</li>
  <li>intelligent interaction design</li>
</ul>

<p>First of all, robotics systems must be able to perceive the semantic entities inherent in the environments as well as the geometry of the environments in order to perform high-level tasks such as errands, cleaning, cooking, and answering questions in addition to navigation. If the systems collect just one type of information among the physical information and semantics of the surrounding environment, they can merely perform simple tasks and cannot provide meaningful services to humans.</p>

<p>Next, versatile manipulation, and recognition of human behavior, intention or message are the very starting point of providing appropriate services to humans. Failing to equip the systems with these capabilities results in irrelevant services—leading to user dissatisfaction. Robootics systems gather information regarding the environment or human through various sensor devices; computationally process the information; detect human behavior, intention, or message; and perform manipulation to realize services.</p>

<p>Further, the natural interaction between humans and robots is becoming more and more important as service robots have started to get deployed in our daily lives. Without natural interaction, robotics systems cannot maximize user satisfaction—missing the very goal of service robots. Natural HRI requires the investigation of human expectation and the reaction of humans, and the process of appropriate designs for intelligent interaction systems in robot-in-the-loop scenarios.</p>

<hr />

<h2 id="multi-modal-perception-and-navigation">Multi-modal perception and navigation</h2>
<h3 id="simvodis-simultaneous-visual-odometry-object-detection-and-instance-segmentation1">SimVODIS: Simultaneous Visual Odometry, Object Detection and Instance Segmentation<sup>[1]</sup></h3>
<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_simvodis.png" alt="SimVODIS Overview" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 1. Overview of the proposed SimVODIS. SimVODIS receives a set of three consecutive images and then estimates semantics (object classes, bounding boxes and object masks), relative poses between input images, and the depth map of the center image.
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a computationally-efficient network architecture to recognize both semantic and geometric information of the surrounding environments. <br />
<strong>[Limitations of Convention]</strong> Conventional approaches combine two techniques to associate geometric and semantic information elicited from the surrounding environments. However, running both algorithms simultaneously requires a lot of computation resources and complicates the software structure. <br />
<strong>[Contribution]</strong> First, we defined a fully data-driven semantic VO algorithm, SimVODIS, for the first time. We expect SimVODIS would provoke the evolution of data-driven VO towards semantic VO/SLAM. Second, we designed the SimVODIS network which simultaneously performs both geometric and semantic tasks. The network conducts multiple tasks utilizing shared feature maps and runs in one thread. Third, we made the source code of the proposed SimVODIS network and the pretrained network parameters open-source.</p>

<h3 id="3-d-scene-graph-a-sparse-and-semantic-representation-of-physical-environments-for-intelligent-agents2">3-D Scene Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents<sup>[2]</sup></h3>
<table class="demo_tb" style="width:50%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<iframe style="width:100%; height:300px;" src="https://www.youtube.com/embed/GXNQAMYU-yQ">
</iframe>
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design an accurate, applicable, usable and scalable environment model for intelligent agents to store observed and perceived environmental information. <br />
<strong>[Limitations of Convention]</strong> Raw and dense representations representing environments with minimum distortion require massive memory space, take much time for processing and lack any semantic information. Moreover, the coverage of abstractive and descriptive representations is confined to the field of view (FoV) of the cameras and the output is in the form of natural language, which makes it difficult for other AI applications to utilize. <br />
<strong>[Contribution]</strong> First, we defined the concept of the 3-D scene graph which represents the environments in an accurate, applicable, usable, and scalable way. Second, we designed the 3-D scene graph construction framework which generates 3-D scene graphs for environments upon receiving a sequence of observations. Third, we provided two application examples of the 3-D scene graph: a) VQA and b) task planning. Fourth, we made the source code of the algorithms open-source.</p>

<h3 id="dual-task-learning-by-leveraging-both-dense-correspondence-and-mis-correspondence-for-robust-change-detection-with-imperfect-matches3">Dual Task Learning by Leveraging Both Dense Correspondence and Mis-Correspondence for Robust Change Detection With Imperfect Matches<sup>[3]</sup></h3>
<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_simsac.gif" alt="SimSaC Demo" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 2. The proposed SimSaC displays robust performance even given imperfect matches of reference and query images with which conventional methods fail.
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a robust scene change detection algorithm given imperfect mathches which are more plausible in the real-world scenarios. <br />
<strong>[Limitations of Convention]</strong> Contemporary approaches for SCD, however, assumes an ideal match of the scene between the current and
the past time steps although observing the same scene with a perfect match hardly occurs in real-world applications. Thus, contemporary approaches would not display the reported performance when deployed to practical systems. <br />
<strong>[Contribution]</strong> First, we carefully formulated a change detection task that reflects performance in realworld settings for the first time. Second, we proposed the SimSaC network for robust change detection which leverages both dense correspondence (scene flow) and miscorrespondence (change). Third, we designed a training scheme that enhances the robustness of change detection without requiring additional annotations. Fourth, we collected a new benchmark dataset consisting of imperfect matches for measuring change detection performance in real-world scenarios. Fifth, we made the source code of the algorithms open-source.</p>

<hr />

<h2 id="service-robots">Service robots</h2>
<h3 id="a-stabilized-feedback-episodic-memory-sf-em-and-home-service-provision-framework-for-robot-and-iot-collaboration4">A Stabilized Feedback Episodic Memory (SF-EM) and Home Service Provision Framework for Robot and IoT Collaboration<sup>[4]<sup></sup></sup></h3>
<table class="demo_tb" style="width:50%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<iframe style="width:100%; height:300px;" src="https://www.youtube.com/embed/7ES-n5MLqmY">
</iframe>
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a computational episodic memory that learns human behaviors and reasons human intentions. <br />
<strong>[Limitations of Convention]</strong> Conventional learning and reasoning algorithms such as hidden Markov model (HMM) or reinforcement learning (RL) either learn in an off-line setting or suffer from the curse of dimensionality. Adaptive theory resonance (ART) networks display instability in long-term scenarios and lack a feedback mechanism. <br />
<strong>[Contribution]</strong> First, we designed a stabilized memory system with a feedback mechanism for incremental learning of human behaviors and reasoning human intentions. Second, we proposed a home service provision framework for robot and IoT collaboration using the proposed SF-ART architecture. Third, we set up a Smart Home environment and verify the effectiveness of both the proposed memory architecture and the service framework.</p>

<hr />

<h2 id="intelligent-interaction-design">Intelligent interaction design</h2>
<h3 id="i-keyboard-fully-imaginary-keyboard-on-touch-devices-empowered-by-deep-neural-decoder5">I-Keyboard: Fully Imaginary Keyboard on Touch Devices Empowered by Deep Neural Decoder<sup>[5]<sup></sup></sup></h3>
<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_I-Keyboard.gif" alt="I-Keyboard Demo" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 3. Users could type on an empty touch screen with ten fingers imagining a keyboard layout. Since users could follow their own typing habit, the usability enhances.
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a natural text-entry method for humans to deliver messages through touch screens (users can start typing using ten fingers on any position at any angle on touch screens without worrying about the keyboard position and shape). <br />
<strong>[Limitations of Convention]</strong> Soft keyboards’ lack of tactile feedback increases the rate of typos and soft keyboards hinder mobile devices from presenting enough content because they occupy a relatively large portion on displays. <br />
<strong>[Contribution]</strong> First, we defined an advanced typing scenario where both a predefined typing area and a calibration step are omitted. Second, we collected user data in an unconstrained environment and comprehensively analyzed user behaviors in such an environment. Third, we designed a deep learning architecture for a shape, position, and angle-independent decoding and formulated the auxiliary loss for training the architecture.</p>

<h3 id="type-anywhere-you-want-an-introduction-to-invisible-mobile-keyboard6">Type Anywhere You Want: An Introduction to Invisible Mobile Keyboard<sup>[6]<sup></sup></sup></h3>
<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_IMK.gif" alt="IMK Demo" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 4. IMK decodes both current and past inputs. As an user writes more text, IMK utilizes more context and the accuracy improves.
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a natural text-entry method for humans to deliver messages through mobile touch screens. <br />
<strong>[Limitations of Convention]</strong> The limitations of soft keyboards for mobile devices include high rate of typos, lack of tactile feedback, inconsideration of users’ different keyboard mental model, and large screen occupation. <br />
<strong>[Contribution]</strong> First, we collected a large-scale, richly annotated IMK dataset which contains approximately 2M pairs of touch points and text. Second, we analyzed the characteristics of user behavior on the new proposed task in-depth. Third, we designed a novel deep neural architecture, Self-Attention Neural Character Decoder (SA-NCD) as a baseline decoder.</p>

<div id="line_breaker" style="clear: both;">
<br />
</div>

<h3 id="writing-in-the-air-unconstrained-text-recognition-from-finger-movement-using-spatio-temporal-convolution7">Writing in The Air: Unconstrained Text Recognition from Finger Movement Using Spatio-Temporal Convolution<sup>[7]<sup></sup></sup></h3>

<table class="demo_tb" style="width:45%; float: left; margin: 5px 15px 0px 0px;">
<tr>
<td style="text-align: center">
<img src="/assets/img/demo_WiTA.gif" alt="WiTA Demo" style="width:100%;" />
</td>
</tr>

<tr>
<td width="45%">
Figure 5. An example instance of the dataset collected in this work. The person in the example is writing "re" from the word "recognized".
</td>
</tr>
</table>

<p><strong>[Research Goal]</strong> Design a computational model that interprets the text written in the air with the index finger using an RGB camera. <br />
<strong>[Limitations of Convention]</strong> Conventional WiTA systems hardly achieve satisfactory performance for practical deployment into real-world applications. Moreover, there is no available benchmark dataset suitable for development and evaluation of WiTA systems. <br />
<strong>[Contribution]</strong> First, we explicitly defined the task of WiTA and collect five types of the dataset in two languages (Korean and English) for the development and evaluation of WiTA systems. Second, we designed baseline methods for the WiTA task and proposed spatio-temporal residual network architectures which translate an input video sequence into a sequence of characters written in the air. Third, we  made the data collection tool, the WiTA dataset, the proposed WiTA baseline networks and the pretrained network parameters open-source.</p>

<hr />

<p><sup> [1] <strong>U.-H. Kim</strong>, S.-H. Kim and J.-H Kim, “SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance Segmentation,” IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 44, no. 1, pp. 428-441, Jan. 2022. [SCIE, IF: 16.390] </sup><br />
<sup> [2] <strong>U.-H. Kim</strong>, J.-M. Park, T.-J. Song and J.-H Kim, “3-D Scene Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents,” IEEE Trans. on Cybernetics, vol. 50, no. 12, pp. 4921-4933, Dec. 2020. [SCIE, IF: 11.079] </sup><br />
<sup> [3] J.-M. Park*, <strong>U.-H. Kim*</strong>, S.-H. Lee and J.-H Kim, “Dual Task Learning by Leveraging Both Dense Correspondence and Mis-Correspondence for Robust Change Detection With Imperfect Matches,” IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13749-13759, Jun. 2022 [Oral Presentation] </sup><br />
<sup> [4] <strong>U.-H. Kim</strong> and J.-H Kim, “A Stabilized Feedback Episodic Memory (SF-EM) and Home Service Provision Framework for Robot and IoT Collaboration,” IEEE Trans. on Cybernetics, vol.  50, no.5, pp.  2110-2123, May 2020. [SCIE, IF: 11.079] </sup><br />
<sup> [5] <strong>U.-H. Kim</strong>, S.-M. Yoo and J.-H Kim, “I-Keyboard: Fully Imaginary Keyboard on Touch Devices Empowered by Deep Neural Decoder,” IEEE Trans. on Cybernetics, Early Access, Sep. 2019. [SCIE, IF: 11.079] </sup><br />
<sup> [6] S.-M. Yoo*, <strong>U.-H. Kim*</strong>, Y.-W. Hwang and J.-H. Kim (* equal contribution), “Type Anywhere You Want: An Introduction to Invisible Mobile Keyboard,” in Proceedings of the 30-th International Joint Conference on Artificial Intelligence (IJCAI), 2021. (13.9% acceptance rate) </sup><br />
<sup> [7] <strong>U.-H. Kim*</strong>, Y.-W. Hwang*, S.-K. Lee and J.-H. Kim (* equal contribution), “Writing in The Air: Unconstrained Text Recognition from Finger Movement Using Spatio-Temporal Convolution,” IEEE Trans. on AI, vol. 4, no. 6, pp. 1386-1398, 2023. </sup><br /></p>

<script>
  if ((window.innerWidth || document.documentElement.clientWidth || document.body.clientWidth) < 600) {
    document.getElementById("line_breaker").innerHTML = "";
    var tables = document.getElementsByClassName("demo_tb");
    for( var i = 0; i < tables.length; i++ ){
        var single_table = tables.item(i);
        single_table.style.width = "100%";
        single_table.style.margin = "0px; 0px; 0px; 0px";
    }
  }
</script>


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2025 ACSL  Autonomous Computing Systems Lab.
    
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
